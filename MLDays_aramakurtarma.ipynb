{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLDays_aramakurtarma.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tknPaAx1HUds",
        "outputId": "ebbc4863-7b56-4761-8a83-0df98ec67ead",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "!wget https://chromedriver.storage.googleapis.com/2.42/chromedriver_linux64.zip  && unzip chromedriver_linux64"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-16 12:02:40--  https://chromedriver.storage.googleapis.com/2.42/chromedriver_linux64.zip\n",
            "Resolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 172.217.204.128, 2607:f8b0:400c:c12::80\n",
            "Connecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|172.217.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4039043 (3.9M) [application/zip]\n",
            "Saving to: ‚Äòchromedriver_linux64.zip‚Äô\n",
            "\n",
            "\rchromedriver_linux6   0%[                    ]       0  --.-KB/s               \rchromedriver_linux6 100%[===================>]   3.85M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-10-16 12:02:40 (151 MB/s) - ‚Äòchromedriver_linux64.zip‚Äô saved [4039043/4039043]\n",
            "\n",
            "Archive:  chromedriver_linux64.zip\n",
            "  inflating: chromedriver            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kahqcDUh94Zk",
        "outputId": "065810f5-3463-412f-cdfa-6b27d56a9e36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "#Mount the drive from Google to save the dataset\n",
        "from google.colab import drive # this will be our driver\n",
        "drive.mount('/gdrive')\n",
        "root = '/gdrive/My Drive/'     # if you want to operate on your Google Drive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hJftl3tl4XM",
        "outputId": "faab8ee6-1684-4e44-cd8c-f1f52ce46e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vexRrk1e3Z5L",
        "outputId": "4da1628f-aa78-4736-e6d8-e332028f028c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "!pip install selenium"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 911kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSEOK1UY-kVP"
      },
      "source": [
        "from selenium import webdriver"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1m17Zio2Yq1",
        "outputId": "e577a985-d4b9-4b69-fb7a-eb7c06287fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "!python scraper.py -page \"haysev\" -len 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"scraper.py\", line 356, in <module>\n",
            "    postBigDict = extract(page=args.page, numOfPost=args.len, infinite_scroll=infinite, scrape_comment=scrape_comment)\n",
            "  File \"scraper.py\", line 256, in extract\n",
            "    browser = webdriver.Chrome(executable_path=\"./chromedriver\", options=option)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/selenium/webdriver/chrome/webdriver.py\", line 73, in __init__\n",
            "    self.service.start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/selenium/webdriver/common/service.py\", line 98, in start\n",
            "    self.assert_process_still_running()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/selenium/webdriver/common/service.py\", line 111, in assert_process_still_running\n",
            "    % (self.path, return_code)\n",
            "selenium.common.exceptions.WebDriverException: Message: Service ./chromedriver unexpectedly exited. Status code was: -6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE7rJgSe4BVY",
        "outputId": "3c0509ae-6ecf-4012-cbb1-2d4125614ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        }
      },
      "source": [
        "pip install facebook-scraper"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting facebook-scraper\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0a/7fffa7e91e1d16389a3bf0ab961bf0995be4e278886770d7de19b2b06db9/facebook_scraper-0.2.13-py3-none-any.whl\n",
            "Collecting requests-html<0.11.0,>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl\n",
            "Collecting html2text<2021.0.0,>=2020.1.16\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/88/14655f727f66b3e3199f4467bafcc88283e6c31b562686bf606264e09181/html2text-2020.1.16-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from requests-html<0.11.0,>=0.10.0->facebook-scraper) (2.23.0)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from requests-html<0.11.0,>=0.10.0->facebook-scraper) (0.0.1)\n",
            "Collecting pyquery\n",
            "  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\n",
            "Collecting pyppeteer>=0.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/4b/3c2aabdd1b91fa52aa9de6cde33b488b0592b4d48efb0ad9efbf71c49f5b/pyppeteer-0.2.2-py3-none-any.whl (145kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 153kB 6.7MB/s \n",
            "\u001b[?25hCollecting parse\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/49/85f19d9ff908817b864deebf7f68211f9a6fc0b48746d372d970f60d01f5/parse-1.18.0.tar.gz\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting w3lib\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook-scraper) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook-scraper) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook-scraper) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook-scraper) (1.24.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->requests-html<0.11.0,>=0.10.0->facebook-scraper) (4.6.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.6/dist-packages (from pyquery->requests-html<0.11.0,>=0.10.0->facebook-scraper) (4.2.6)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting websockets<9.0,>=8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 7.8MB/s \n",
            "\u001b[?25hCollecting tqdm<5.0.0,>=4.42.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/cf/f91813073e4135c1183cadf968256764a6fe4e35c351d596d527c0540461/tqdm-4.50.2-py2.py3-none-any.whl (70kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71kB 6.5MB/s \n",
            "\u001b[?25hCollecting pyee<8.0.0,>=7.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/64/f3/90db6276dbc6cb1defa558251acc73c8e436ca8e1e2b38ec75786278de7c/pyee-7.0.4-py2.py3-none-any.whl\n",
            "Collecting appdirs<2.0.0,>=1.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib->requests-html<0.11.0,>=0.10.0->facebook-scraper) (1.15.0)\n",
            "Building wheels for collected packages: parse, fake-useragent\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.18.0-cp36-none-any.whl size=24133 sha256=8041a501e91ab63f88049dfc5f9de32c83783cd7384a430af201ad40e12407f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/53/09/869ca5781ede342254ffac09ca99461b008c3e5f8dd079b0c0\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=2ea76e85ecb5e6af2d544137a5b66f2fd12d52972824ea4267e1532303be0369\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "Successfully built parse fake-useragent\n",
            "\u001b[31mERROR: pyppeteer 0.2.2 has requirement urllib3<2.0.0,>=1.25.8, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cssselect, pyquery, websockets, tqdm, pyee, appdirs, pyppeteer, parse, fake-useragent, w3lib, requests-html, html2text, facebook-scraper\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed appdirs-1.4.4 cssselect-1.1.0 facebook-scraper-0.2.13 fake-useragent-0.1.11 html2text-2020.1.16 parse-1.18.0 pyee-7.0.4 pyppeteer-0.2.2 pyquery-1.4.1 requests-html-0.10.0 tqdm-4.50.2 w3lib-1.22.0 websockets-8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBgCG_OrDZGb"
      },
      "source": [
        "!facebook-scraper --filename etuhayvanseverler.csv 'etuhayvanseverler' --pages 100\n",
        "!facebook-scraper --filename haysev.csv 'haysev' --pages 100"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yhHT0geYvIM"
      },
      "source": [
        "!facebook-scraper --filename hayvanseverlerbilkent.csv 'hayvanseverlerbilkent' --pages 100\n",
        "!facebook-scraper --filename buhst.csv 'buhst' --pages 100\n",
        "!facebook-scraper --filename sivashaysev.csv 'sivashaysev' --pages 100"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkyT9ZvCbfV7"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "corpus = []\n",
        "for root,dirs,files in os.walk('/content/csv_data'):\n",
        "    for file in files:\n",
        "      if file.endswith(\".csv\"):\n",
        "        data = pd.read_csv('/content/csv_data/'+file)\n",
        "        for i in range(0,len(data)):\n",
        "          corpus.append(data['text'][i])"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVm9Pbl4VVzM",
        "outputId": "ac4ca7a7-9345-4464-ef6f-5229d99931c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "corpus[1]"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'üè° ANKARA / ACIÃáL YUVA ARANIYOR üè°\\nBu yavrumuzun kardesÃßlerine koÃàpekler saldƒ±rdƒ±gÃÜƒ± ve bulundugÃÜu yer guÃàvensiz oldugÃÜu icÃßin gecÃßici yuvaya alƒ±ndƒ±. Daha sonra sahiplendirildi, fakat sahiplenne ki≈üiler miyavladƒ±ƒüƒ± gerek√ßesiyle ge√ßici yuvasƒ±na geri bƒ±rakmƒ±≈ülar. Karma a≈üƒ±larƒ± ve i√ß-dƒ±≈ü parazitleri olan 3-4 aylƒ±k bu g√ºzel oƒülumuz onu sevecek ve hi√ß terk etmeyecek yuvasƒ±nƒ± arƒ±yor. CÃßok oyuncu, cana yakƒ±n ve tam bir ev kedisi. Ona yuva olmak isterseniz a≈üaƒüƒ±daki numara ile ileti≈üime ge√ßebilirsiniz.\\n\\nIÃáletisÃßim: Ezgi Altƒ±noÃàz\\nüìû: +90 (539) 869 04 24'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGNL49sujAtt"
      },
      "source": [
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "for i,line in enumerate(corpus): #removes whitespaces, emojis and stopwords\n",
        "  if isinstance(line, str):\n",
        "    line = emoji_pattern.sub(r'', line)\n",
        "    stop_words = set(stopwords.words('turkish')) \n",
        "  \n",
        "    word_tokens = word_tokenize(line) \n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "    line = ' '.join(filtered_sentence)\n",
        "\n",
        "    line = ' '.join(line.split())\n",
        "    corpus[i] = line\n",
        "    "
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59-zyGrajqEZ",
        "outputId": "d61b226a-ba38-4e0d-efb9-1a1a9aaf4628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "corpus[1]"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ANKARA / ACIÃáL YUVA ARANIYOR Bu yavrumuzun kardesÃßlerine koÃàpekler saldƒ±rdƒ±gÃÜƒ± bulundugÃÜu yer guÃàvensiz oldugÃÜu icÃßin gecÃßici yuvaya alƒ±ndƒ± . Daha sonra sahiplendirildi , fakat sahiplenne ki≈üiler miyavladƒ±ƒüƒ± gerek√ßesiyle ge√ßici yuvasƒ±na geri bƒ±rakmƒ±≈ülar . Karma a≈üƒ±larƒ± i√ß-dƒ±≈ü parazitleri olan 3-4 aylƒ±k g√ºzel oƒülumuz onu sevecek terk etmeyecek yuvasƒ±nƒ± arƒ±yor . CÃßok oyuncu , cana yakƒ±n tam bir ev kedisi . Ona yuva olmak isterseniz a≈üaƒüƒ±daki numara ileti≈üime ge√ßebilirsiniz . IÃáletisÃßim : Ezgi Altƒ±noÃàz : +90 ( 539 ) 869 04 24'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8A2wIJDgnVW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}